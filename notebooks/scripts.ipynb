{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c76e5c9",
   "metadata": {},
   "source": [
    "# üê¶ EfficientNetB2 Bird Species Classifier\n",
    "\n",
    "This notebook prepares a bird species image dataset, builds an image classification model using EfficientNetB2, and trains the model using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d52903c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "!{sys.executable} -m pip install aiohttp aiofiles pandas requests matplotlib torch torchvision torchinfo tqdm ipywidgets\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecd7351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import aiohttp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from config.config import (\n",
    "\tTEXAS_BIRDS_CSV,\n",
    "\tTAXA_URL,\n",
    "\tPAGES_TO_FETCH,\n",
    "\tDATASET_PATH,\n",
    "\tRAW_DATA_PATH,\n",
    "\tMODEL_SAVE_PATH,\n",
    "\tRESULTS_SAVE_PATH,\n",
    "\tBATCH_SIZE,\n",
    "\tNUM_WORKERS,\n",
    "\tSEED,\n",
    "\tTRAIN_SPLIT_RATIO,\n",
    "\tNUM_CLASSES,\n",
    "\tLEARNING_RATE,\n",
    "\tEPOCHS,\n",
    "\tEARLY_STOPPING_PATIENCE,\n",
    "\tEARLY_STOPPING_DELTA,\n",
    ")\n",
    "\n",
    "from scripts.fetch_taxon_id import get_taxon_id_by_name\n",
    "from scripts.fetch_images import process_species\n",
    "from scripts.rename_files import rename_image_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7379aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "birds_df = pd.read_csv(TEXAS_BIRDS_CSV)\n",
    "taxon_ids = []\n",
    "\n",
    "for bird in birds_df[\"bird_name\"]:\n",
    "  print(f\"Fetching taxon_id for '{bird}'\")\n",
    "  taxon_id = get_taxon_id_by_name(TAXA_URL, bird)\n",
    "  taxon_ids.append(taxon_id)\n",
    "  time.sleep(1) # 1 second sleep to follow API rate limits\n",
    "\n",
    "birds_df.insert(1, column = \"taxon_id\", value = taxon_ids)\n",
    "birds_df.to_csv(TEXAS_BIRDS_CSV, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d033484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def download_images():\n",
    "  all_licensing = []\n",
    "  global_semaphore = asyncio.Semaphore(1)\n",
    "\n",
    "  async with aiohttp.ClientSession() as session:\n",
    "    tasks = []\n",
    "    for index, row in birds_df.iterrows():\n",
    "      bird_name = row[\"bird_name\"]\n",
    "      taxon_id = row[\"taxon_id\"]\n",
    "      tasks.append(process_species(session, bird_name, taxon_id, PAGES_TO_FETCH, RAW_DATA_PATH, global_semaphore))\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    for metadata in results:\n",
    "      all_licensing.extend(metadata)\n",
    "\n",
    "  licensing_csv = os.path.join(RAW_DATA_PATH, \"attribution_metadata.csv\")\n",
    "  pd.DataFrame(all_licensing).to_csv(licensing_csv, index=False)\n",
    "  print(f\"Licensing metadata saved to {licensing_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb82d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "await download_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d1e6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_image_files_all_folders(directory: str) -> None:\n",
    "\tfor root, dirs, _ in os.walk(directory):\n",
    "\t\tfor d in dirs:\n",
    "\t\t\tfolder_path = os.path.join(root, d)\n",
    "\t\t\tprint(f\"üîç Processing: {folder_path}\")\n",
    "\t\t\trename_image_files(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9a4b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_image_files_all_folders(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88e387b",
   "metadata": {},
   "source": [
    "# splitting images into train and test folders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b05cd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(original_data_dir: str, output_base_dir: str, train_ratio: float = 0.8):\n",
    "\t\"\"\"\n",
    "\tSplits a dataset of images organized by class folders into training and testing sets.\n",
    "\tEach subdirectory in the original dataset represents a class (e.g., a bird species),\n",
    "\tand contains image files. This function randomly splits each class's images into\n",
    "\ttraining and testing subsets according to the given ratio, and copies them into\n",
    "\tcorresponding 'train' and 'test' folders under the output directory.\n",
    "\tArgs:\n",
    "\t\toriginal_data_dir (str): Path to the original dataset directory. Must contain\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tsubdirectories for each class/species.\n",
    "\t\toutput_base_dir (str): Path to the base directory where the 'train' and 'test'\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tfolders will be created.\n",
    "\t\ttrain_ratio (float, optional): Proportion of images to use for training.\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tRemaining images will be used for testing.\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tDefaults to 0.8.\n",
    "\tRaises:\n",
    "\t\tAssertionError: If original_data_dir is not a valid directory.\n",
    "\tReturns:\n",
    "\t\tNone\n",
    "\t\"\"\"\n",
    "\tassert os.path.isdir(original_data_dir), f\"{original_data_dir} is not a valid directory\"\n",
    "\n",
    "\tspecies_folders = [f for f in os.listdir(original_data_dir) if os.path.isdir(os.path.join(original_data_dir, f))]\n",
    "\n",
    "\tfor species in species_folders:\n",
    "\t\tspecies_path = os.path.join(original_data_dir, species)\n",
    "\t\timages = [f for f in os.listdir(species_path)\n",
    "\t\t\t\t\t\t\tif os.path.isfile(os.path.join(species_path, f))\n",
    "\t\t\t\t\t\t\tand not (f.startswith('.') or f == \"DS_Store\")]\n",
    "\n",
    "\t\trandom.shuffle(images)\n",
    "\t\ttrain_count = int(len(images) * train_ratio)\n",
    "\t\ttrain_images = images[:train_count]\n",
    "\t\ttest_images = images[train_count:]\n",
    "\n",
    "\t\t# output species directories\n",
    "\t\ttrain_species_dir = os.path.join(output_base_dir, \"train\", species)\n",
    "\t\ttest_species_dir = os.path.join(output_base_dir, \"test\", species)\n",
    "\t\tos.makedirs(train_species_dir, exist_ok=True)\n",
    "\t\tos.makedirs(test_species_dir, exist_ok=True)\n",
    "\n",
    "\t\t# copy files into new directories\n",
    "\t\tfor img in train_images:\n",
    "\t\t\tshutil.copy2(os.path.join(species_path, img), os.path.join(train_species_dir, img))\n",
    "\t\tfor img in test_images:\n",
    "\t\t\tshutil.copy2(os.path.join(species_path, img), os.path.join(test_species_dir, img))\n",
    "\n",
    "\t\tprint(f\"Done splitting {species}: {len(train_images)} train / {len(test_images)} test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac70113d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split(RAW_DATA_PATH, DATASET_PATH, TRAIN_SPLIT_RATIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7178748",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_image_files_all_folders(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db5c1f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf5ee4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_effnetb2_model(num_classes:int, seed:int=42):\n",
    "\t\"\"\"\n",
    "\tCreates a pretrained EfficientNetB2 model adapted for custom image classification,\n",
    "\talong with appropriate training and validation transforms.\n",
    "\tThis function:\n",
    "\t- Loads EfficientNetB2 with ImageNet weights.\n",
    "\t- Replaces the final classification layer to match the number of target classes.\n",
    "\t- Returns a model ready for fine-tuning (all layers are trainable).\n",
    "\t- Provides data augmentation transforms for training.\n",
    "\t- Provides ImageNet-style transforms for validation.\n",
    "\tArgs\n",
    "\t\t\tnum_classes (int, optional): Number of output classes for classification.\n",
    "\t\t\tseed (int, optional): Random seed for reproducible initialization of\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tthe final classifier layer. Defaults to 42.\n",
    "\tReturns:\n",
    "\t\t\tTuple[\n",
    "\t\t\t\t\ttorch.nn.Module,          # The EfficientNetB2 model with modified classifier\n",
    "\t\t\t\t\ttorchvision.transforms.Compose,  # Transformations for training data\n",
    "\t\t\t\t\ttorchvision.transforms.Compose   # Transformations for validation data\n",
    "\t\t\t]\n",
    "  \"\"\"\n",
    "\tweights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
    "\n",
    "\ttrain_transforms = transforms.Compose([\n",
    "\ttransforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "\ttransforms.RandomHorizontalFlip(p=0.5),\n",
    "\ttransforms.RandomRotation(degrees=15),\n",
    "\ttransforms.ToTensor(),\n",
    "\ttransforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tstd=[0.229, 0.224, 0.225])\n",
    "\t])\n",
    "\n",
    "\tval_transforms = weights.transforms()\n",
    "\tmodel = torchvision.models.efficientnet_b2(weights=weights)\n",
    "\n",
    "\tfor p in model.features.parameters():\n",
    "\t\tp.requires_grad = True\n",
    "\n",
    "\ttorch.manual_seed(seed)\n",
    "\tmodel.classifier = nn.Sequential(\n",
    "\t\tnn.Dropout(p=0.3, inplace=True),\n",
    "\t\tnn.Linear(in_features=1408, out_features=num_classes)\n",
    "\t)\n",
    "\n",
    "\treturn model, train_transforms, val_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8714c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "effnetb2, train_tf, val_tf = create_effnetb2_model(num_classes=NUM_CLASSES, seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b53f5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "effnetb2 = effnetb2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44bd944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
       "============================================================================================================================================\n",
       "EfficientNet (EfficientNet)                                  [1, 3, 224, 224]     [1, 35]              --                   True\n",
       "‚îú‚îÄSequential (features)                                      [1, 3, 224, 224]     [1, 1408, 7, 7]      --                   True\n",
       "‚îÇ    ‚îî‚îÄConv2dNormActivation (0)                              [1, 3, 224, 224]     [1, 32, 112, 112]    --                   True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d (0)                                       [1, 3, 224, 224]     [1, 32, 112, 112]    864                  True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d (1)                                  [1, 32, 112, 112]    [1, 32, 112, 112]    64                   True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSiLU (2)                                         [1, 32, 112, 112]    [1, 32, 112, 112]    --                   --\n",
       "‚îÇ    ‚îî‚îÄSequential (1)                                        [1, 32, 112, 112]    [1, 16, 112, 112]    --                   True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (0)                                       [1, 32, 112, 112]    [1, 16, 112, 112]    1,448                True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (1)                                       [1, 16, 112, 112]    [1, 16, 112, 112]    612                  True\n",
       "‚îÇ    ‚îî‚îÄSequential (2)                                        [1, 16, 112, 112]    [1, 24, 56, 56]      --                   True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (0)                                       [1, 16, 112, 112]    [1, 24, 56, 56]      6,004                True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (1)                                       [1, 24, 56, 56]      [1, 24, 56, 56]      10,710               True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (2)                                       [1, 24, 56, 56]      [1, 24, 56, 56]      10,710               True\n",
       "‚îÇ    ‚îî‚îÄSequential (3)                                        [1, 24, 56, 56]      [1, 48, 28, 28]      --                   True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (0)                                       [1, 24, 56, 56]      [1, 48, 28, 28]      16,518               True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (1)                                       [1, 48, 28, 28]      [1, 48, 28, 28]      43,308               True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (2)                                       [1, 48, 28, 28]      [1, 48, 28, 28]      43,308               True\n",
       "‚îÇ    ‚îî‚îÄSequential (4)                                        [1, 48, 28, 28]      [1, 88, 14, 14]      --                   True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (0)                                       [1, 48, 28, 28]      [1, 88, 14, 14]      50,300               True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (1)                                       [1, 88, 14, 14]      [1, 88, 14, 14]      123,750              True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (2)                                       [1, 88, 14, 14]      [1, 88, 14, 14]      123,750              True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (3)                                       [1, 88, 14, 14]      [1, 88, 14, 14]      123,750              True\n",
       "‚îÇ    ‚îî‚îÄSequential (5)                                        [1, 88, 14, 14]      [1, 120, 14, 14]     --                   True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (0)                                       [1, 88, 14, 14]      [1, 120, 14, 14]     149,158              True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (1)                                       [1, 120, 14, 14]     [1, 120, 14, 14]     237,870              True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (2)                                       [1, 120, 14, 14]     [1, 120, 14, 14]     237,870              True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (3)                                       [1, 120, 14, 14]     [1, 120, 14, 14]     237,870              True\n",
       "‚îÇ    ‚îî‚îÄSequential (6)                                        [1, 120, 14, 14]     [1, 208, 7, 7]       --                   True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (0)                                       [1, 120, 14, 14]     [1, 208, 7, 7]       301,406              True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (1)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       686,868              True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (2)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       686,868              True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (3)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       686,868              True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (4)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       686,868              True\n",
       "‚îÇ    ‚îî‚îÄSequential (7)                                        [1, 208, 7, 7]       [1, 352, 7, 7]       --                   True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (0)                                       [1, 208, 7, 7]       [1, 352, 7, 7]       846,900              True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (1)                                       [1, 352, 7, 7]       [1, 352, 7, 7]       1,888,920            True\n",
       "‚îÇ    ‚îî‚îÄConv2dNormActivation (8)                              [1, 352, 7, 7]       [1, 1408, 7, 7]      --                   True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d (0)                                       [1, 352, 7, 7]       [1, 1408, 7, 7]      495,616              True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d (1)                                  [1, 1408, 7, 7]      [1, 1408, 7, 7]      2,816                True\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSiLU (2)                                         [1, 1408, 7, 7]      [1, 1408, 7, 7]      --                   --\n",
       "‚îú‚îÄAdaptiveAvgPool2d (avgpool)                                [1, 1408, 7, 7]      [1, 1408, 1, 1]      --                   --\n",
       "‚îú‚îÄSequential (classifier)                                    [1, 1408]            [1, 35]              --                   True\n",
       "‚îÇ    ‚îî‚îÄDropout (0)                                           [1, 1408]            [1, 1408]            --                   --\n",
       "‚îÇ    ‚îî‚îÄLinear (1)                                            [1, 1408]            [1, 35]              49,315               True\n",
       "============================================================================================================================================\n",
       "Total params: 7,750,309\n",
       "Trainable params: 7,750,309\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 657.69\n",
       "============================================================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 156.80\n",
       "Params size (MB): 31.00\n",
       "Estimated Total Size (MB): 188.40\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(effnetb2, \n",
    "        input_size=(1, 3, 224, 224),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df05194b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = Path(DATASET_PATH) / \"train\"\n",
    "test_dir = Path(DATASET_PATH) / \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4c6243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(\n",
    "  train_dir: str, \n",
    "  test_dir: str, \n",
    "  train_transform: transforms.Compose,\n",
    "  val_transform:  transforms.Compose,\n",
    "  batch_size: int, \n",
    "  num_workers: int = NUM_WORKERS\n",
    "):\n",
    "  train_ds = datasets.ImageFolder(train_dir, transform=train_transform)\n",
    "  val_ds = datasets.ImageFolder(test_dir, transform=val_transform)\n",
    "  class_names = train_ds.classes\n",
    "\n",
    "  train_dataloader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=False\n",
    "  )\n",
    "  test_dataloader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=False\n",
    "  )\n",
    "\n",
    "  return train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59a0ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader_effnetb2, test_dataloader_effnetb2, class_names = create_dataloaders(train_dir=train_dir,\n",
    "                                                                                    test_dir=test_dir,\n",
    "                                                                                    train_transform=train_tf,\n",
    "                                                                                    val_transform=val_tf,\n",
    "                                                                                    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852f27f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "\t\"\"\"\n",
    "\tSimple early‚Äêstopper that checkpoints the best model and\n",
    "\tstops when val_loss hasn‚Äôt improved for `patience` epochs.\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, patience:int=3, delta:float=0.0, path:str=\"best_model.pth\"):\n",
    "\t\t\"\"\"\n",
    "\t\tArgs:\n",
    "\t\t\tpatience: how many epochs to wait after last time val_loss improved\n",
    "\t\t\tdelta: minimum change in val_loss to qualify as an improvement\n",
    "\t\t\tpath: where to save the best model weights\n",
    "\t\t\"\"\"\n",
    "\t\tself.patience = patience\n",
    "\t\tself.delta = delta\n",
    "\t\tself.path = path\n",
    "\n",
    "\t\tself.best_loss = np.inf\n",
    "\t\tself.num_bad_epochs = 0\n",
    "\n",
    "\tdef __call__(self, val_loss:float, model:torch.nn.Module):\n",
    "\t\t# save and reset counter if loss improved by at least delta \n",
    "\t\tif val_loss + self.delta < self.best_loss:\n",
    "\t\t\tself.best_loss = val_loss\n",
    "\t\t\tself.num_bad_epochs = 0\n",
    "\t\t\ttorch.save(model.state_dict(), self.path)\n",
    "\t\t\tprint(f\"  ‚Ü≥ val_loss improved to {val_loss:.4f}, saving model.\")\n",
    "\t\telse:\n",
    "\t\t\tself.num_bad_epochs += 1\n",
    "\t\t\tprint(f\"  ‚Ü≥ no improvement ({self.num_bad_epochs}/{self.patience})\")\n",
    "\n",
    "\t\t# return True if we‚Äôve hit patience\n",
    "\t\treturn self.num_bad_epochs >= self.patience\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38003c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "\n",
    "def train_step(model: torch.nn.Module, \n",
    "               dataloader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               device: torch.device) -> Tuple[float, float]:\n",
    "\t\n",
    "  model.train()\n",
    "  train_loss, train_acc = 0, 0\n",
    "  for batch, (X, y) in enumerate(dataloader):\n",
    "      X, y = X.to(device), y.to(device)\n",
    "      y_pred = model(X)\n",
    "      loss = loss_fn(y_pred, y)\n",
    "      train_loss += loss.item() \n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "      train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
    "  train_loss = train_loss / len(dataloader)\n",
    "  train_acc = train_acc / len(dataloader)\n",
    "  return train_loss, train_acc\n",
    "\n",
    "def test_step(model: torch.nn.Module, \n",
    "              dataloader: torch.utils.data.DataLoader, \n",
    "              loss_fn: torch.nn.Module,\n",
    "              device: torch.device) -> Tuple[float, float]:\n",
    "  model.eval() \n",
    "  test_loss, test_acc = 0, 0\n",
    "  with torch.inference_mode():\n",
    "      for batch, (X, y) in enumerate(dataloader):\n",
    "          X, y = X.to(device), y.to(device)\n",
    "          test_pred_logits = model(X)\n",
    "          loss = loss_fn(test_pred_logits, y)\n",
    "          test_loss += loss.item()\n",
    "          test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "          test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
    "  test_loss = test_loss / len(dataloader)\n",
    "  test_acc = test_acc / len(dataloader)\n",
    "  return test_loss, test_acc\n",
    "\n",
    "def train(model: torch.nn.Module, \n",
    "          train_dataloader: torch.utils.data.DataLoader, \n",
    "          test_dataloader: torch.utils.data.DataLoader, \n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module,\n",
    "          epochs: int,\n",
    "          early_stopper: EarlyStopping,\n",
    "          device: torch.device) -> Dict[str, List]:\n",
    "    results = {\"train_loss\": [],\n",
    "                \"train_acc\": [],\n",
    "                \"test_loss\": [],\n",
    "                \"test_acc\": []}\n",
    "    for epoch in range(1, epochs+1):\n",
    "        # 1) train + test\n",
    "        train_loss, train_acc = train_step(model, train_dataloader, loss_fn, optimizer, device)\n",
    "        test_loss, test_acc   = test_step(model, test_dataloader, loss_fn, device)\n",
    "\n",
    "        # 2) log\n",
    "        print(f\"Epoch {epoch}/{epochs} | \"\n",
    "                f\"train_loss: {train_loss:.4f} | train_acc: {train_acc:.4f} | \"\n",
    "                f\"test_loss: {test_loss:.4f} | test_acc: {test_acc:.4f}\")\n",
    "\n",
    "        # 3) record history\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "        # 4) early stop?\n",
    "        if early_stopper is not None:\n",
    "            stop = early_stopper(test_loss, model)\n",
    "            if stop:\n",
    "                print(f\"Stopped early at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "    # restore best weights if we used early stopping\n",
    "    if early_stopper is not None:\n",
    "        model.load_state_dict(torch.load(early_stopper.path))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f58e65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "\n",
    "def train_step(model: torch.nn.Module, \n",
    "               dataloader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               device: torch.device) -> Tuple[float, float]:\n",
    "\t\"\"\"\n",
    "\tPerforms one training epoch on the given dataloader.\n",
    "\n",
    "\tArgs:\n",
    "\t\tmodel (nn.Module): The PyTorch model to train.\n",
    "\t\tdataloader (DataLoader): DataLoader for the training dataset.\n",
    "\t\tloss_fn (nn.Module): Loss function to use.\n",
    "\t\toptimizer (Optimizer): Optimizer for updating model weights.\n",
    "\t\tdevice (torch.device): Device to perform computations on (e.g., 'cuda' or 'cpu').\n",
    "\n",
    "\tReturns:\n",
    "\t\tTuple[float, float]: Average training loss and accuracy for the epoch.\n",
    "\t\"\"\"\n",
    "\tmodel.train()\n",
    "\ttrain_loss, train_acc = 0.0, 0.0\n",
    "\n",
    "\tfor batch, (X, y) in enumerate(dataloader):\n",
    "\t\tX, y = X.to(device), y.to(device)\n",
    "\t\ty_pred = model(X)\n",
    "\t\tloss = loss_fn(y_pred, y)\n",
    "\t\ttrain_loss += loss.item()\n",
    "\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\ty_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "\t\ttrain_acc += (y_pred_class == y).sum().item() / len(y_pred)\n",
    "\n",
    "\ttrain_loss /= len(dataloader)\n",
    "\ttrain_acc /= len(dataloader)\n",
    "\treturn train_loss, train_acc\n",
    "\n",
    "\n",
    "def test_step(model: torch.nn.Module, \n",
    "              dataloader: torch.utils.data.DataLoader, \n",
    "              loss_fn: torch.nn.Module,\n",
    "              device: torch.device) -> Tuple[float, float]:\n",
    "\t\"\"\"\n",
    "\tEvaluates the model on the test/validation dataset.\n",
    "\n",
    "\tArgs:\n",
    "\t\tmodel (nn.Module): The PyTorch model to evaluate.\n",
    "\t\tdataloader (DataLoader): DataLoader for the test/validation dataset.\n",
    "\t\tloss_fn (nn.Module): Loss function to use.\n",
    "\t\tdevice (torch.device): Device to perform computations on.\n",
    "\n",
    "\tReturns:\n",
    "\t\tTuple[float, float]: Average test loss and accuracy for the epoch.\n",
    "\t\"\"\"\n",
    "\tmodel.eval()\n",
    "\ttest_loss, test_acc = 0.0, 0.0\n",
    "\n",
    "\twith torch.inference_mode():\n",
    "\t\tfor batch, (X, y) in enumerate(dataloader):\n",
    "\t\t\tX, y = X.to(device), y.to(device)\n",
    "\t\t\ttest_pred_logits = model(X)\n",
    "\t\t\tloss = loss_fn(test_pred_logits, y)\n",
    "\t\t\ttest_loss += loss.item()\n",
    "\n",
    "\t\t\ttest_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "\t\t\ttest_acc += (test_pred_labels == y).sum().item() / len(test_pred_labels)\n",
    "\n",
    "\ttest_loss /= len(dataloader)\n",
    "\ttest_acc /= len(dataloader)\n",
    "\treturn test_loss, test_acc\n",
    "\n",
    "\n",
    "def train(model: torch.nn.Module, \n",
    "          train_dataloader: torch.utils.data.DataLoader, \n",
    "          test_dataloader: torch.utils.data.DataLoader, \n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module,\n",
    "          epochs: int,\n",
    "          early_stopper: EarlyStopping,\n",
    "          device: torch.device) -> Dict[str, List]:\n",
    "\t\"\"\"\n",
    "\tTrains and evaluates the model over multiple epochs, with optional early stopping.\n",
    "\tArgs:\n",
    "\t\tmodel (nn.Module): The model to train.\n",
    "\t\ttrain_dataloader (DataLoader): Dataloader for training data.\n",
    "\t\ttest_dataloader (DataLoader): Dataloader for test/validation data.\n",
    "\t\toptimizer (Optimizer): Optimizer for training.\n",
    "\t\tloss_fn (nn.Module): Loss function to optimize.\n",
    "\t\tepochs (int): Number of epochs to train.\n",
    "\t\tearly_stopper: EarlyStopping object to monitor validation loss and stop early if needed.\n",
    "\t\tdevice (torch.device): Device to use for training and evaluation.\n",
    "\tReturns:\n",
    "\t\tDict[str, List[float]]: Dictionary containing lists of training/test loss and accuracy per epoch.\n",
    "\t\"\"\"\n",
    "\tresults = {\n",
    "\t\t\"train_loss\": [],\n",
    "\t\t\"train_acc\": [],\n",
    "\t\t\"test_loss\": [],\n",
    "\t\t\"test_acc\": []\n",
    "\t}\n",
    "\n",
    "\tfor epoch in range(1, epochs + 1):\n",
    "\t\t# 1: train and evaluate\n",
    "\t\ttrain_loss, train_acc = train_step(model, train_dataloader, loss_fn, optimizer, device)\n",
    "\t\ttest_loss, test_acc = test_step(model, test_dataloader, loss_fn, device)\n",
    "\n",
    "\t\t# 2: log metrics\n",
    "\t\tprint(f\"Epoch {epoch}/{epochs} | \"\n",
    "\t\t\t\t\tf\"train_loss: {train_loss:.4f} | train_acc: {train_acc:.4f} | \"\n",
    "\t\t\t\t\tf\"test_loss: {test_loss:.4f} | test_acc: {test_acc:.4f}\")\n",
    "\n",
    "\t\t# 3: save metrics\n",
    "\t\tresults[\"train_loss\"].append(train_loss)\n",
    "\t\tresults[\"train_acc\"].append(train_acc)\n",
    "\t\tresults[\"test_loss\"].append(test_loss)\n",
    "\t\tresults[\"test_acc\"].append(test_acc)\n",
    "\n",
    "\t\t# 4: early stopping check\n",
    "\t\tif early_stopper is not None:\n",
    "\t\t\tstop = early_stopper(test_loss, model)\n",
    "\t\t\tif stop:\n",
    "\t\t\t\tprint(f\"Early stopping at epoch {epoch}\")\n",
    "\t\t\t\tbreak\n",
    "                  \n",
    "\t# best model\n",
    "\tif early_stopper is not None:\n",
    "\t\tmodel.load_state_dict(torch.load(early_stopper.path))\n",
    "\n",
    "\treturn results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77046195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | train_loss: 1.2877 | train_acc: 0.6714 | test_loss: 0.3332 | test_acc: 0.9041\n",
      "  ‚Ü≥ val_loss improved to 0.3332, saving model.\n",
      "Epoch 2/10 | train_loss: 0.4425 | train_acc: 0.8668 | test_loss: 0.2447 | test_acc: 0.9214\n",
      "  ‚Ü≥ val_loss improved to 0.2447, saving model.\n",
      "Epoch 3/10 | train_loss: 0.3034 | train_acc: 0.9073 | test_loss: 0.2071 | test_acc: 0.9319\n",
      "  ‚Ü≥ val_loss improved to 0.2071, saving model.\n",
      "Epoch 4/10 | train_loss: 0.2247 | train_acc: 0.9301 | test_loss: 0.2122 | test_acc: 0.9296\n",
      "  ‚Ü≥ no improvement (1/3)\n",
      "Epoch 5/10 | train_loss: 0.1714 | train_acc: 0.9466 | test_loss: 0.1976 | test_acc: 0.9377\n",
      "  ‚Ü≥ val_loss improved to 0.1976, saving model.\n",
      "Epoch 6/10 | train_loss: 0.1417 | train_acc: 0.9552 | test_loss: 0.1857 | test_acc: 0.9390\n",
      "  ‚Ü≥ val_loss improved to 0.1857, saving model.\n",
      "Epoch 7/10 | train_loss: 0.1132 | train_acc: 0.9646 | test_loss: 0.1837 | test_acc: 0.9430\n",
      "  ‚Ü≥ val_loss improved to 0.1837, saving model.\n",
      "Epoch 8/10 | train_loss: 0.1000 | train_acc: 0.9680 | test_loss: 0.1826 | test_acc: 0.9448\n",
      "  ‚Ü≥ val_loss improved to 0.1826, saving model.\n",
      "Epoch 9/10 | train_loss: 0.0815 | train_acc: 0.9739 | test_loss: 0.1748 | test_acc: 0.9482\n",
      "  ‚Ü≥ val_loss improved to 0.1748, saving model.\n",
      "Epoch 10/10 | train_loss: 0.0676 | train_acc: 0.9785 | test_loss: 0.1722 | test_acc: 0.9497\n",
      "  ‚Ü≥ val_loss improved to 0.1722, saving model.\n"
     ]
    }
   ],
   "source": [
    "optimizer_finetune = torch.optim.Adam(params=effnetb2.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "stopper = EarlyStopping(\n",
    "\tpatience=EARLY_STOPPING_PATIENCE,\n",
    "\tdelta=EARLY_STOPPING_DELTA,\n",
    "\tpath=MODEL_SAVE_PATH\n",
    ")\n",
    "\n",
    "# model training\n",
    "effnetb2_results = train(\n",
    "    model=effnetb2,\n",
    "    train_dataloader=train_dataloader_effnetb2,\n",
    "    test_dataloader=test_dataloader_effnetb2,\n",
    "    epochs=EPOCHS,\n",
    "    optimizer=optimizer_finetune,\n",
    "    loss_fn=loss_fn,\n",
    "    device=device,\n",
    "    early_stopper=stopper\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339998dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save training results\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "with open(RESULTS_SAVE_PATH, \"w\") as f:\n",
    "    json.dump(effnetb2_results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
